#!/usr/bin/env python3
import sys
import subprocess
import time
import requests
import socket
import json
import argparse
import os
from pathlib import Path

LMSTUDIO_CMD = ["lmstudio", "--model", "/path/to/model", "--api"]
LMSTUDIO_URL = "http://127.0.0.1:1234/v1/chat/completions"
CONVERSATION_FILE = "conversation_history.json"

def load_conversation_history():
    """Load the last 5 messages from conversation history"""
    if not os.path.exists(CONVERSATION_FILE):
        return []
    
    try:
        with open(CONVERSATION_FILE, 'r', encoding='utf-8') as f:
            history = json.load(f)
        # Return only the last 5 messages (excluding system message)
        return history[-5:] if len(history) > 5 else history
    except (json.JSONDecodeError, FileNotFoundError):
        return []

def save_conversation_history(messages):
    """Save conversation history to file"""
    try:
        with open(CONVERSATION_FILE, 'w', encoding='utf-8') as f:
            json.dump(messages, f, indent=2, ensure_ascii=False)
    except Exception as e:
        print(f"Warning: Could not save conversation history: {e}")

def is_server_running(host="127.0.0.1", port=1234):
    with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
        try:
            s.connect((host, port))
            return True
        except:
            return False

def start_lmstudio():
    proc = subprocess.Popen(LMSTUDIO_CMD, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)
    time.sleep(5)
    return proc

def query_lmstudio(user_prompt):
    # Load conversation history
    conversation_history = load_conversation_history()
    
    # Build messages array with system prompt, history, and new user message
    messages = [
        {"role": "system", "content": "You answer all the questions in short and precise manner."}
    ]
    
    # Add conversation history (excluding system messages from history)
    for msg in conversation_history:
        if msg.get("role") != "system":
            messages.append(msg)
    
    # Add current user message
    current_user_msg = {"role": "user", "content": user_prompt}
    messages.append(current_user_msg)

    headers = {"Content-Type": "application/json"}
    data = {
        "model": "dolphin3.0-llama3.1-8b",
        "messages": messages,
        "temperature": 0.8,
        "max_tokens": 300,
        "stream": True
    }

    try:
        assistant_response = ""
        with requests.post(LMSTUDIO_URL, headers=headers, json=data, stream=True) as response:
            if response.status_code != 200:
                print(f"Error: {response.status_code} - {response.text}")
                return

            for line in response.iter_lines(decode_unicode=True):
                if not line:
                    continue
                line = line.strip()
                if line.startswith("data: "):
                    payload = line[len("data: "):]
                    if payload == "[DONE]":
                        break
                    try:
                        decoded = json.loads(payload)
                        delta = decoded.get("choices", [{}])[0].get("delta", {}).get("content", "")
                        if delta:
                            assistant_response += delta
                            print(delta, end="", flush=True)
                    except json.JSONDecodeError:
                        continue
            print()  # newline at end
        
        # Update conversation history with new messages
        conversation_history.append(current_user_msg)
        conversation_history.append({"role": "assistant", "content": assistant_response})
        
        # Keep only the last 5 pairs (10 messages total: 5 user + 5 assistant)
        # or adjust as needed for your memory requirements
        if len(conversation_history) > 10:
            conversation_history = conversation_history[-10:]
        
        # Save updated history
        save_conversation_history(conversation_history)
        
    except Exception as e:
        print("Error querying LMStudio:", e)

def clear_history():
    """Clear conversation history"""
    try:
        if os.path.exists(CONVERSATION_FILE):
            os.remove(CONVERSATION_FILE)
            print("Conversation history cleared.")
        else:
            print("No conversation history to clear.")
    except Exception as e:
        print(f"Error clearing history: {e}")

def show_history():
    """Display current conversation history"""
    history = load_conversation_history()
    if not history:
        print("No conversation history found.")
        return
    
    print("Current conversation history:")
    print("-" * 40)
    for i, msg in enumerate(history, 1):
        role = msg.get("role", "unknown").upper()
        content = msg.get("content", "")
        print(f"{i}. {role}: {content[:100]}{'...' if len(content) > 100 else ''}")
    print("-" * 40)

def main():
    parser = argparse.ArgumentParser(description="Alex - LMStudio CLI with conversation memory")
    parser.add_argument('--clear', action='store_true', help='Clear conversation history')
    parser.add_argument('--history', action='store_true', help='Show conversation history')
    parser.add_argument('prompt', nargs=argparse.REMAINDER, help="Your question or prompt")
    args = parser.parse_args()

    # Handle special commands
    if args.clear:
        clear_history()
        return
    
    if args.history:
        show_history()
        return

    if not args.prompt:
        print("Usage: alex Your question here")
        print("       alex --clear (to clear history)")
        print("       alex --history (to show history)")
        return

    user_prompt = " ".join(args.prompt)

    if not is_server_running():
        print("Starting LMStudio server...")
        start_lmstudio()

    query_lmstudio(user_prompt)

if __name__ == "__main__":
    main()

